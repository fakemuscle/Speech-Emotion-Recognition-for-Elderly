{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10012573,"sourceType":"datasetVersion","datasetId":6164315},{"sourceId":11265208,"sourceType":"datasetVersion","datasetId":6893780},{"sourceId":11283149,"sourceType":"datasetVersion","datasetId":7054464}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"name":"notebook27966acb8e","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nimport io\nimport soundfile as sf\nfrom IPython.display import Audio\n\nimport torchaudio\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments, Wav2Vec2ForSequenceClassification, Wav2Vec2Model, Wav2Vec2PreTrainedModel, AutoProcessor, AutoModelForCTC\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport random\n\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nnum_of_folds = 3","metadata":{"id":"p8Ryn6lUzA0C","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:36:07.877953Z","iopub.execute_input":"2025-04-07T15:36:07.8782Z","iopub.status.idle":"2025-04-07T15:36:32.414369Z","shell.execute_reply.started":"2025-04-07T15:36:07.87817Z","shell.execute_reply":"2025-04-07T15:36:32.41369Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extracting Files","metadata":{}},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"label = []\npath = []\nfor dir_name, _, file_list in os.walk(\"/kaggle/input/elderly-speech-emotion-recognition\"):\n    \n    for file in file_list:\n        # print(file)\n        label.append(dir_name.split('_')[-1].lower())\n        path.append(os.path.join(dir_name, file))\ndf = pd.DataFrame()\ndf[\"path\"] = path\ndf[\"label\"] = label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:36:32.415146Z","iopub.execute_input":"2025-04-07T15:36:32.415665Z","iopub.status.idle":"2025-04-07T15:36:32.489233Z","shell.execute_reply.started":"2025-04-07T15:36:32.415639Z","shell.execute_reply":"2025-04-07T15:36:32.488611Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## YueMotion Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nyue_ds = load_dataset(\"CAiRE/YueMotion\")\nyue_df_list = []\nfor i in yue_ds:\n    yue_df_list.append(yue_ds[f\"{i}\"].to_pandas())\nyue_df = pd.concat(yue_df_list)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:36:32.489995Z","iopub.execute_input":"2025-04-07T15:36:32.4902Z","iopub.status.idle":"2025-04-07T15:36:35.462198Z","shell.execute_reply.started":"2025-04-07T15:36:32.49018Z","shell.execute_reply":"2025-04-07T15:36:35.46122Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the Audios","metadata":{}},{"cell_type":"code","source":"def bytes_to_audio(bytes):\n    bytes = bytes[\"bytes\"]\n    return sf.read(io.BytesIO(bytes))[0]\n    # print(bytes)\n    \nyue_df[\"audio\"] = yue_df[\"audio\"].map(bytes_to_audio)\ndf_audio = []\nfor i in range(len(df)):\n    df_audio.append(librosa.load(df[\"path\"].iloc[i], sr=16000)[0])\ndf.insert(loc=len(df.keys()), column=\"audio\", value=df_audio)\ndf.insert(loc=len(df.keys()), column=\"group\", value=[\"elderly\"]*len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:36:35.4644Z","iopub.execute_input":"2025-04-07T15:36:35.464632Z","iopub.status.idle":"2025-04-07T15:36:50.740511Z","shell.execute_reply.started":"2025-04-07T15:36:35.464613Z","shell.execute_reply":"2025-04-07T15:36:50.739824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting Audios into Adult and Elderly","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:36:50.742025Z","iopub.execute_input":"2025-04-07T15:36:50.742816Z","iopub.status.idle":"2025-04-07T15:36:50.746032Z","shell.execute_reply.started":"2025-04-07T15:36:50.742789Z","shell.execute_reply":"2025-04-07T15:36:50.745186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def classify(x):\n    if x>54:\n        return \"elderly\"\n    else:\n        return \"adult\"\n# yue_df = yue_df.drop(yue_df[yue_df.age<54].index)\nyue_df.insert(loc=len(yue_df.keys()), column=\"group\", value=[classify(yue_df[\"age\"].iloc[x]) for x in range(len(yue_df))])\nyue_df = yue_df.drop(columns=[\"split\", \"path\", \"age\", \"speaker_id\", \"gender\", \"sentence_id\", \"label_id\"], axis=1)\nyue_df = yue_df.drop(yue_df[yue_df.label==\"disgust\"].index)\ndf.drop(columns=[\"path\"], axis=1)\n# df = pd.concat([yue_df, df])\n# df = yue_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:36:50.746794Z","iopub.execute_input":"2025-04-07T15:36:50.747158Z","iopub.status.idle":"2025-04-07T15:36:50.820589Z","shell.execute_reply.started":"2025-04-07T15:36:50.747126Z","shell.execute_reply":"2025-04-07T15:36:50.819643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df)\n# 176 362 895","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:36:50.821425Z","iopub.execute_input":"2025-04-07T15:36:50.82173Z","iopub.status.idle":"2025-04-07T15:36:50.840402Z","shell.execute_reply.started":"2025-04-07T15:36:50.821698Z","shell.execute_reply":"2025-04-07T15:36:50.839594Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"code","source":"def generalize(x):\n    if x == \"angry\":\n        return \"anger\"\n    return x\n\nemotions = list(df['label'].unique())\nEtoI = {label: ind for ind, label in enumerate(df['label'].unique())}\nItoE = {i: l for l, i in EtoI.items()}\ndf['label'] = df['label'].map(generalize)\ndf['label'] = df['label'].map(EtoI)","metadata":{"id":"4xgH_MOGzA0D","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:36:50.841275Z","iopub.execute_input":"2025-04-07T15:36:50.841491Z","iopub.status.idle":"2025-04-07T15:36:50.857853Z","shell.execute_reply.started":"2025-04-07T15:36:50.841471Z","shell.execute_reply":"2025-04-07T15:36:50.857084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MixedDataset(Dataset):\n    def __init__(self, df, processor, max_length=160000):\n        self.df = df\n        self.processor = processor\n        self.max_length = max_length\n        # self.audio = [processor(librosa.load(df[\"path\"].iloc[i])[0], sampling_rate=16000, return_tensors=\"pt\", padding=True, max_length=max_length) for i in range(len(df))]\n        # self.audio = [processor()]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, ind):\n        label = self.df.iloc[ind]['label']\n        speech = self.df.iloc[ind]['audio']\n        text_feature = self.df.iloc[ind]['text_rep']\n\n        if len(speech)>self.max_length:\n            speech = speech[:self.max_length]\n        else:\n            speech = np.pad(speech, (0, self.max_length-len(speech)), 'constant')\n\n        audios = self.processor(speech, sampling_rate=16000, return_tensors='pt', padding=True, max_length=self.max_length)\n        # if self.audio[ind] == 0:\n        #     self.audio[ind] = processor(librosa.load(df[\"path\"].iloc[ind])[0], sampling=16000, return_tensors=\"pt\", padding=True, max_length=self.max_length)\n        # input_val = self.audio[ind].input_values.squeeze()\n        # label = self.df[\"label\"].iloc[ind]\n        audio_feature = audios.input_values.squeeze()\n        text_feature = torch.from_numpy(text_feature)\n        \n        input_val = torch.cat((audio_feature, text_feature))\n        # input_val = audio_feature\n        return {'input_values':input_val, 'labels':torch.tensor(label, dtype=torch.long)}","metadata":{"id":"V2v9YfIUzA0E","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:36:50.858572Z","iopub.execute_input":"2025-04-07T15:36:50.858829Z","iopub.status.idle":"2025-04-07T15:36:50.869198Z","shell.execute_reply.started":"2025-04-07T15:36:50.858807Z","shell.execute_reply":"2025-04-07T15:36:50.868375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing Pre-trained Models","metadata":{}},{"cell_type":"code","source":"# Classifier for Wav2Vec2\nfrom sklearn.utils.class_weight import compute_class_weight\nclass Wav2Vec2ClassificationHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n        x = features\n        # x = self.relu(x)\n        x = self.dense(x)\n        # x = self.dropout(x)\n        x = self.relu(x)\n        # x = self.dense(x)\n        # x = torch.tanh(x)\n        # x = self.dropout(x)\n        x = self.out_proj(x)\n        return x\n\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=range(5) , y=df[\"label\"])\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device=\"cuda\")\n\nclass Wav2Vec2ForClassification(Wav2Vec2PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.wav2vec2 = Wav2Vec2Model(config)\n        num_layers = config.num_hidden_layers + 1  # transformer layers + input embeddings\n        if config.use_weighted_layer_sum:\n            self.layer_weights = nn.Parameter(torch.ones(num_layers) / num_layers)\n            \n        self.classifier = Wav2Vec2ClassificationHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def freeze_feature_extractor(self):\n        warnings.warn(\n            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n            FutureWarning,\n        )\n        self.freeze_feature_encoder()\n\n    def freeze_feature_encoder(self):\n        self.wav2vec2.feature_extractor._freeze_parameters()\n\n    def freeze_base_model(self):\n        for param in self.wav2vec2.parameters():\n            param.requires_grad = False\n\n    def forward(\n        self,\n        input_values,\n        attention_mask = None,\n        output_attentions = None,\n        output_hidden_states = None,\n        return_dict = None,\n        labels = None,\n    ):\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.wav2vec2(\n            input_values,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        pooled_output = hidden_states.mean(dim=1)\n\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            \n            loss_fct = CrossEntropyLoss(weight=class_weights)\n            # loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:36:50.870028Z","iopub.execute_input":"2025-04-07T15:36:50.870247Z","iopub.status.idle":"2025-04-07T15:36:51.138621Z","shell.execute_reply.started":"2025-04-07T15:36:50.870227Z","shell.execute_reply":"2025-04-07T15:36:51.137953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# acoustic model for speech emotion recognition\nprocessor = AutoProcessor.from_pretrained(\"ctl/wav2vec2-large-xlsr-cantonese\")\nmodels = [Wav2Vec2ForClassification.from_pretrained(\"ctl/wav2vec2-large-xlsr-cantonese\", num_labels=5) for i in range(num_of_folds)]","metadata":{"id":"cLrwbrgqzA0F","outputId":"37180f45-2592-49ea-f020-79fb74efaeda","trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-04-07T15:36:51.139356Z","iopub.execute_input":"2025-04-07T15:36:51.139608Z","iopub.status.idle":"2025-04-07T15:37:01.810852Z","shell.execute_reply.started":"2025-04-07T15:36:51.139585Z","shell.execute_reply":"2025-04-07T15:37:01.809883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# speech to text model for text features\nfrom transformers import pipeline\ndevice = \"cuda\"\nMODEL_NAME = \"alvanlii/whisper-small-cantonese\" \nlang = \"zh\"\npipe = pipeline(\n    task=\"automatic-speech-recognition\",\n    model=MODEL_NAME,\n    chunk_length_s=30,\n    device=device,\n)\npipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(language=lang, task=\"transcribe\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:37:01.812383Z","iopub.execute_input":"2025-04-07T15:37:01.812639Z","iopub.status.idle":"2025-04-07T15:37:10.646622Z","shell.execute_reply.started":"2025-04-07T15:37:01.812614Z","shell.execute_reply":"2025-04-07T15:37:10.645685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fasttext\nft = fasttext.load_model(\"/kaggle/input/canto-fasttext/toastynews.bin\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:37:10.650965Z","iopub.execute_input":"2025-04-07T15:37:10.651183Z","iopub.status.idle":"2025-04-07T15:37:26.919622Z","shell.execute_reply.started":"2025-04-07T15:37:10.651164Z","shell.execute_reply":"2025-04-07T15:37:26.918924Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Accuire Text Representation of Audio","metadata":{}},{"cell_type":"code","source":"text = [pipe(df.iloc[i][\"audio\"])[\"text\"] for i in range(len(df))]\ntext_rep = [ft.get_sentence_vector(x) for x in text]\ndf[\"text_rep\"] = text_rep\npipe = None\nft = None\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:37:26.921041Z","iopub.execute_input":"2025-04-07T15:37:26.92135Z","iopub.status.idle":"2025-04-07T15:39:22.423463Z","shell.execute_reply.started":"2025-04-07T15:37:26.921313Z","shell.execute_reply":"2025-04-07T15:39:22.422777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Data for K-Fold Cross-Validation","metadata":{}},{"cell_type":"code","source":"fold = StratifiedKFold(n_splits=num_of_folds, random_state=42, shuffle=True)\ntrain_df_arr= []\ntest_df_arr = []\nfor train_ind, test_ind in fold.split(df[df[\"group\"]==\"elderly\"], df[df[\"group\"]==\"elderly\"][\"label\"]):\n    train_df_arr.append(pd.concat([df[df[\"group\"]==\"elderly\"].iloc[train_ind], df[df[\"group\"]==\"adult\"]]))\n    test_df_arr.append(df[df[\"group\"]==\"elderly\"].iloc[test_ind])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:39:22.424435Z","iopub.execute_input":"2025-04-07T15:39:22.424743Z","iopub.status.idle":"2025-04-07T15:39:22.440838Z","shell.execute_reply.started":"2025-04-07T15:39:22.424711Z","shell.execute_reply":"2025-04-07T15:39:22.440134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"train_dataset_arr = [MixedDataset(x, processor) for x in train_df_arr]\ntest_dataset_arr = [MixedDataset(x, processor) for x in test_df_arr]","metadata":{"id":"x8F_usdMzA0F","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:39:22.441595Z","iopub.execute_input":"2025-04-07T15:39:22.441809Z","iopub.status.idle":"2025-04-07T15:39:22.456297Z","shell.execute_reply.started":"2025-04-07T15:39:22.441788Z","shell.execute_reply":"2025-04-07T15:39:22.455654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n\ndef compute_metrics(pred):\n  labels = pred.label_ids\n  preds = np.argmax(pred.predictions, axis=1)\n  accuracy = accuracy_score(labels, preds)\n  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n  s_labels = [ItoE[x] for x in labels]\n  s_preds = [ItoE[x] for x in preds]\n  conf_matrix = confusion_matrix(s_labels, s_preds, labels=list(EtoI.keys()))\n  return {\n      'accuracy': accuracy,\n      'precision': precision,\n      'recall': recall,\n      'f1': f1,\n      'confusion_matrix': conf_matrix.tolist(),\n  }","metadata":{"id":"4iicm4AA5x3M","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:39:22.457254Z","iopub.execute_input":"2025-04-07T15:39:22.457491Z","iopub.status.idle":"2025-04-07T15:39:22.473417Z","shell.execute_reply.started":"2025-04-07T15:39:22.45747Z","shell.execute_reply":"2025-04-07T15:39:22.472669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',\n    logging_strategy='epoch',\n    eval_strategy='epoch',\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    report_to=[]\n)","metadata":{"id":"VdIOknsh3h6R","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:39:22.474267Z","iopub.execute_input":"2025-04-07T15:39:22.47456Z","iopub.status.idle":"2025-04-07T15:39:22.516493Z","shell.execute_reply.started":"2025-04-07T15:39:22.474529Z","shell.execute_reply":"2025-04-07T15:39:22.515915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res = []\nfor i in range(num_of_folds):\n    train_data = DataLoader(train_dataset_arr[i], batch_size=8, shuffle=True).dataset\n    test_data = DataLoader(test_dataset_arr[i], batch_size=8, shuffle=True).dataset\n    trainer = Trainer(\n        model=models[i],\n        args=training_args,\n        train_dataset=train_data,\n        eval_dataset=test_data,\n        compute_metrics=compute_metrics,\n        tokenizer=processor.feature_extractor,\n    )\n    trainer.train()\n    res.append(trainer.evaluate())\n    trainer = None\n    train_data = None\n    test_data = None\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"id":"pAsv0jNH6BF3","outputId":"33c5422e-3d46-4519-a44f-ec623d58e075","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T15:39:22.517136Z","iopub.execute_input":"2025-04-07T15:39:22.51732Z","execution_failed":"2025-04-07T16:07:16.131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay\nemotions = list(EtoI.keys())\nind = 0\nfor x in res:\n    np_confusion_matrix = np.array(x['eval_confusion_matrix'])\n    conf_matrix_plot = ConfusionMatrixDisplay(confusion_matrix=np_confusion_matrix, display_labels=emotions)\n    conf_matrix_plot.plot()\n    plt.savefig(f'confusion_matrix_{ind}.png')\n    ind = ind+1\n# plt.savefig('plot.png')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-07T16:07:16.132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_accuracy = 0\nfor i in res:\n    mean_accuracy += i[\"eval_accuracy\"]\nmean_accuracy /= num_of_folds\nprint(mean_accuracy)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-07T16:07:16.132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for ind, x in enumerate(res):\n    print(\n        f\"Fold {ind}:\",\n        f\"Eval Accuracy: {x['eval_accuracy']}\",\n        f\"Eval F1 Score: {x['eval_f1']}\",\n        sep=\"\\n\"\n    )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-07T16:07:16.132Z"}},"outputs":[],"execution_count":null}]}